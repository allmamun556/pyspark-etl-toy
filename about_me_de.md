

# **Professionelle Version – Deutsch**

### **Berufliche Version**

Ich habe meinen Masterabschluss in **Data Science** an der Berliner Hochschule für Technik (Berlin University of Applied Sciences) erworben. Zuvor absolvierte ich meinen Bachelorabschluss in **Information and Communication Technology** in Bangladesch.

Während meiner Zeit in Deutschland arbeitete ich für **drei Unternehmen**. Meine erste Position war bei **Backpacker Trail in Kaiserslautern**, wo ich als **Data Engineer** tätig war. Meine Hauptverantwortung bestand darin, **Datenpipelines** zu entwerfen und zu entwickeln – insbesondere **ETL-Pipelines**, die Daten aus verschiedenen Quellen extrahierten, diese transformierten und bereinigten und anschließend in neue Datenbanken für Analysezwecke luden.

Ich arbeitete häufig mit **API-basierter Datenextraktion**, transformierte Rohdaten in aussagekräftige Formate und entwickelte End-to-End-Pipelines, die hochwertige Daten für verschiedene Anwendungsfälle leicht zugänglich machten. Dazu gehörten auch die Entwicklung automatisierter Workflows, das Monitoring der Pipeline-Performance und die Sicherstellung der Datenqualität.

Neben der Pipeline-Entwicklung trug ich auch zur **Überwachung und Visualisierung** wichtiger Leistungskennzahlen bei. Ich erstellte Dashboards mit Tools wie **Grafana, Plotly, Power BI und Tableau**. Beispielsweise baute ich Power-BI-Dashboards zur Überwachung wichtiger KPIs wie der beliebtesten Reiseziele auf der Plattform, Orte mit der höchsten Nutzeraktivität oder das allgemeine Nutzungsverhalten.

Darüber hinaus unterstützte ich die Entwicklung von **Machine-Learning-Lösungen**, darunter Kundensegmentierungsmodelle mittels Clustering-Techniken. Diese Modelle basierten auf Parametern wie Standort und Nutzerverhalten.

Während dieser Projekte arbeitete ich mit einer Vielzahl von Technologien, darunter:

* **Programmierung & Datenverarbeitung:** Python
* **Datenbanken:** PostgreSQL, SQL
* **Visualisierung:** Power BI, Tableau, Grafana, Plotly, Bokeh, Streamlit
* **Frameworks & Bibliotheken:** Django, Streamlit und weitere Tools für die gesamte Pipeline-Entwicklung

Diese Kombination aus technischer Expertise und praktischer Projekterfahrung ermöglichte es mir, vollständige, skalierbare und zuverlässige Datenlösungen für Analytik, Business Insights und Machine-Learning-Anwendungen zu entwickeln.

---

### **Berufliche Version – John Deere European Innovation Center**

Anschließend arbeitete ich etwa neun Monate lang im **John Deere European Innovation Center** als **Data Scientist**. In dieser Rolle trug ich zu einer Vielzahl von Data-Science- und Data-Engineering-Projekten bei, darunter **Machine-Learning-Modellentwicklung, ETL-Pipeline-Aufbau und Datenvisualisierung**.

Ein wesentlicher Teil meiner Arbeit bestand aus dem Aufbau von **End-to-End-ETL-Pipelines**. Mithilfe der offiziellen **John Deere APIs** extrahierte ich Daten aus mehreren internen und externen Quellen. Ich führte umfangreiche Datenbereinigungen durch, überprüfte Datenkonsistenz, identifizierte Unstimmigkeiten und deren Ursachen und entwickelte neue Features entsprechend den Projektanforderungen. Die verarbeiteten Daten wurden anschließend in neuen Datenbanken für spätere Analysen und Modellierungen gespeichert.

Während meiner Tätigkeit bei John Deere arbeitete ich mit verschiedenen Datentypen, darunter:

* **Rasterdaten**
* **Bilddaten**
* **Sensor- und Telemetriedaten** von Landmaschinen wie Traktoren und Mähdreschern

Ich bereinigte, transformierte und harmonisierte diese Datensätze, bevor ich sie in strukturierte Speichersysteme lud. Dabei sammelte ich praktische Erfahrungen mit Cloud-basierter Infrastruktur, darunter:

* **Amazon S3** (mit Partitionierung für große Datenmengen)
* **Amazon Redshift** (als Data Warehouse)
* **PostgreSQL** (für relationale Analysen)

Neben der Pipeline-Entwicklung entwickelte ich **Machine-Learning-Modelle** für Forecasting-, Clustering- und Klassifizierungsaufgaben. Beispielsweise baute ich **räumliche Regressions- und Interpolationsmodelle**, um Feld- und Pflanzenzustände wie Pflanzenwachstum, Bodenfeuchtigkeit oder Erntebereitschaft zu überwachen. Diese Modelle unterstützten Landwirte und interne Teams bei Entscheidungen zur Ressourcenplanung und Ernteoptimierung.

Ich entwickelte zudem **Power-BI-Dashboards**, um wichtige Indikatoren wie Pflanzenzustandstrends zu überwachen. Diese Dashboards verwandelten komplexe Rohdaten aus John Deere Maschinen in nutzbare, verständliche Einblicke.

Da die von den Maschinen generierten Daten oft extrem komplex, unstrukturiert und verrauscht waren, bestand ein Teil meiner täglichen Arbeit darin, robuste ETL-Pipelines zu entwickeln, die diese Herausforderungen bewältigen konnten. Dazu gehörte:

* Entfernen von Ausreißern
* Normalisierung und Standardisierung
* Räumliches Filtern
* Einsatz von Clustering-Methoden wie **k-Means** und **DBSCAN** zur Muster- oder Anomalieerkennung

Insgesamt vereinte meine Rolle **Data Engineering, Machine Learning und Visualisierung**, um datengetriebene Lösungen für die Präzisionslandwirtschaft zu unterstützen.

---

### **Berufliche Version – Enertrag SE (Aktuelle Position)**

Derzeit arbeite ich als **Data Science Specialist** bei **Enertrag SE**, wo ich an einer Vielzahl von Projekten im Bereich Data Science, Data Engineering und Analytics beteiligt bin. Mein Fokus liegt hauptsächlich auf der Analyse und Optimierung von **Windkraftanlagen** und **Photovoltaikanlagen**.

Eine meiner wichtigsten Aufgaben ist die Analyse der Leistungsdaten von Windturbinen, um festzustellen, welche Anlagen die niedrigste Effizienz aufweisen und welche die höchste Energieproduktion liefern. Ich entwickle **Performance-Dashboards** für bestimmte Windparks und führe detaillierte Untersuchungen zu den **Wartungsmustern** einzelner Turbinen durch – etwa welche Art von Wartung erforderlich ist, wann sie durchgeführt werden sollte und ob frühzeitige Anzeichen von Degradation vorliegen. Diese Erkenntnisse unterstützen die Wartungsteams bei der rechtzeitigen Durchführung von Maßnahmen.

Als Data Scientist entwickle ich verschiedene **Machine-Learning-Modelle** zur Turbinenleistung, darunter:

* **Clustering-Modelle** zur Gruppierung von Turbinen nach Leistungscharakteristik
* **Anomalieerkennungsmodelle**, z. B. zur Erkennung von Temperaturabweichungen
* **Vorhersagemodelle**, die die Energieproduktion der nächsten Stunde prognostizieren

Windturbinen sind mit zahlreichen Sensoren ausgestattet, die große Datenmengen erzeugen. Ein wesentlicher Teil meiner Arbeit besteht darin, **Anomalien wie extreme Temperatursprünge** zu erkennen, die häufig auf Fehlfunktionen hinweisen. Ich entwickle dynamische Modelle, die solche Anomalien frühzeitig erkennen können.

Ich arbeite auch an **Energieprognosemodellen**, die vorhersagen, wie viel Energie eine Turbine in naher Zukunft produzieren wird. Diese Modelle werden mithilfe moderner Cloud-Technologien entwickelt, getestet und produktiv gesetzt.

Aus Data-Engineering-Perspektive entwickle ich **Daten-Connectoren**, die Daten aus verschiedenen Turbinenkomponenten und Sensoren sammeln. Ich führe Datenbereinigung, Konsistenzprüfungen und Ausreißerentfernung durch und speichere die bereinigten Daten anschließend in neuen Datenbanken. Dadurch wird sichergestellt, dass ML-Modelle und Dashboards auf zuverlässigen Daten basieren.

Zu den verwendeten Technologien gehören:

* **Python, SQL**
* **CI/CD-Pipelines**
* **AWS-Dienste**
* **Versionskontrolle und Deployment-Tools**

Ich entwickle zudem Visualisierungen und Berichte, um die Turbinenleistung zu überwachen, Probleme frühzeitig zu erkennen und datenbasierte Entscheidungen zu ermöglichen.

Insgesamt vereint meine tägliche Arbeit den Aufbau von ML-Modellen, den Aufbau von Datenpipelines, die Entwicklung von Dashboards und die Sicherstellung hochwertiger Energiedaten. Die Arbeit bei Enertrag SE ist für mich besonders spannend, da sie direkt zur Optimierung erneuerbarer Energien beiträgt.

---

### **Berufliche Version – Agile Arbeitsweise & Teamarbeit**

Als Data Scientist arbeite ich in einem **agilen Umfeld** und folge etablierten Agile-Methoden während des gesamten Entwicklungsprozesses. Wir arbeiten in **Sprints**, in denen Aufgaben klar definiert und zugewiesen werden. Unser Team nutzt **Jira-Boards**, um Aufgaben zu organisieren, Fortschritte zu verfolgen und eine reibungslose Zusammenarbeit sicherzustellen.

Ein wichtiger Bestandteil unserer Arbeitsweise ist die **Codequalität**. Wir führen **Peer Code Reviews** durch, geben uns gegenseitig Feedback und befolgen Best Practices, um hohe Qualitätsstandards zu gewährleisten. Dieser Prozess trägt zu kontinuierlicher Verbesserung und Wissensaustausch im Team bei.

Ich arbeite sowohl **selbstständig** als auch **teamorientiert**, je nach Bedarf der Aufgaben. In der Zusammenarbeit lege ich großen Wert auf offene und transparente Kommunikation. Ich diskutiere gern Ideen, analysiere Vor- und Nachteile verschiedener Ansätze und treffe gemeinsam Entscheidungen, die für das Team und das Unternehmen am besten sind.

Ich möchte immer ein positives Arbeitsumfeld schaffen und versuche, **Konflikte zu vermeiden**, indem ich Anliegen frühzeitig kläre, aktiv zuhöre und konstruktiv zusammenarbeite.

---

### **Berufliche Version – Motivation & Hintergrund**

Mein akademischer Hintergrund konzentriert sich vollständig auf **Daten**. Sowohl während meines Studiums als auch in meiner beruflichen Laufbahn habe ich eine starke Leidenschaft für die Arbeit mit Daten entwickelt. Meine gesamte Erfahrung dreht sich um datenbezogene Bereiche, und ich arbeite sehr gern an **Machine-Learning-Modellen**, **Datenmodellen**, **ETL-Pipelines** und **Dashboards bzw. Visualisierungsberichten**.

Ich bin besonders motiviert von Aufgaben, bei denen ich tief mit Daten arbeiten kann – sei es in der Analyse, im Engineering oder im Machine Learning. Es macht mir Freude, komplexe Datensätze zu erforschen, reale Probleme zu lösen und sinnvolle datengetriebene Lösungen zu entwickeln. Diese Leidenschaft treibt mich an, mich kontinuierlich weiterzuentwickeln und Rollen zu suchen, in denen ich meine Fähigkeiten erweitern und durch Daten echten Mehrwert schaffen kann.

---


