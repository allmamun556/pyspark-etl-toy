
### **Professional Version**

I completed my Master’s degree in **Data Science** from the Berlin University of Applied Sciences. Before that, I earned my Bachelor’s degree in **Information and Communication Technology** in Bangladesh.

During my time in Germany, I worked for **three companies**. My first role was at **Backpacker Trail in Kaiserslautern**, where I worked as a **Data Engineer**. My primary responsibility was to design and build **data pipelines**, specifically **ETL pipelines** that extracted data from various sources, transformed and cleaned the data, and then loaded it into new databases for analytics.

I frequently worked with **API-based data extraction**, transforming raw data into meaningful formats, and building end-to-end pipelines that made high-quality data easily accessible to team members for different use cases. This included designing automated workflows, monitoring pipeline performance, and ensuring data quality.

In addition to pipeline development, I also contributed to the **monitoring and visualization** of key performance metrics. I created dashboards using tools such as **Grafana, Plotly, Power BI, and Tableau**. For example, I built Power BI dashboards to monitor key performance indicators such as the most popular tourist destinations on the platform, locations with the highest user engagement, and overall platform activity.

I also supported the development of **machine learning solutions**, including customer segmentation models using clustering techniques. These models were built based on various parameters such as location and user behavior.

Throughout these projects, I worked with a broad range of technologies, including:

* **Programming & Data Processing:** Python
* **Databases:** PostgreSQL, SQL
* **Visualization Tools:** Power BI, Tableau, Grafana, Plotly, Bokeh, Streamlit
* **Frameworks & Libraries:** Django, Streamlit, and others used for full data pipeline development

This combination of technical expertise and hands-on project experience allowed me to build complete, scalable, and reliable data solutions for analytics, business insights, and machine learning use cases.


---

### **Professional Version – John Deere European Innovation Center**

After that, I worked at the **John Deere European Innovation Center** as a **Data Scientist** for approximately nine months. In this role, I contributed to a variety of data science and data engineering projects, including **machine learning model development, ETL pipeline construction, and data visualization**.

A major part of my work involved building **end-to-end ETL pipelines**. Using the official **John Deere APIs**, I extracted data from multiple internal and external sources. I performed extensive data cleaning, checked for data consistency, identified inconsistencies along with their root causes, and engineered new features based on project requirements. After processing, I stored the cleaned and transformed data in new databases for downstream analytics and modeling.

During my time at John Deere, I worked with several different types of datasets, including:

* **Raster data**
* **Image data**
* **Sensor and telemetry data** from agricultural machinery such as tractors and combine harvesters

I cleaned, transformed, and harmonized these datasets before loading them into structured storage systems. I gained hands-on experience with cloud-based data infrastructure, including:

* **Amazon S3** (with partitioning for large-scale data storage)
* **Amazon Redshift** (for data warehousing)
* **PostgreSQL** (for relational storage and analytics)

In addition to pipeline work, I developed **machine learning models** for forecasting, clustering, and classification tasks. For example, I built **spatial regression and interpolation models** to monitor crop and field conditions—such as crop health, soil moisture, and harvest readiness. These models helped farmers and internal teams better understand field conditions and make informed decisions for harvesting and resource planning.

I also created **Power BI dashboards** to monitor key indicators, including crop health metrics and field condition trends. These dashboards transformed complex raw data from John Deere machinery into actionable insights.

The raw data from John Deere machines was often extremely complex, noisy, and unstructured. As part of my daily responsibilities, I designed robust ETL pipelines to handle these challenges. This included:

* Removing outliers
* Normalizing and standardizing data
* Performing spatial filtering
* Applying clustering techniques such as **k-means** and **DBSCAN** to detect patterns or anomalies

Overall, my role combined **data engineering, machine learning, and visualization**, enabling data-driven solutions for precision agriculture and supporting the innovation team with reliable, scalable data products.


---

### **Professional Version – Enertrag SE (Current Role)**

I am currently working as a **Data Science Specialist** at **Enertrag SE**, where I am involved in a wide range of data science, data engineering, and analytics projects. My work focuses primarily on the analysis and optimization of **wind turbines** and **solar energy systems**.

One of my main responsibilities is to analyze performance data from wind turbines to identify which assets have the lowest performance efficiency and which ones show the highest output. I build **performance analysis dashboards** for specific wind parks and conduct detailed investigations into the **maintenance patterns** of individual wind turbines. This includes identifying what type of maintenance is required, when it should be performed, and detecting early signs of system degradation. My insights support maintenance teams in taking timely and effective actions.

As a Data Scientist, I develop a variety of **machine learning models** related to wind-turbine performance, including:

* **Clustering models** for grouping wind turbines based on performance characteristics
* **Anomaly detection models**, such as detecting temperature anomalies in turbine components
* **Predictive models** to estimate the energy production for the next hour for a specific turbine

Wind turbines are equipped with numerous sensors, each generating large volumes of data. A critical part of my role is detecting anomalies, such as **extreme temperature spikes**, which often indicate component malfunction. I build dynamic models that can detect these anomalies in advance, helping prevent system failures.

I also work on models for **energy forecasting**, predicting how much energy a wind turbine is likely to produce in the near future. These ML models are developed, tested, and deployed into production using various cloud-based technologies.

From a data engineering perspective, I build **data connectors** that collect data from different turbine components and sensors. I perform data cleaning, consistency checks, outlier removal, and then store the processed data in new databases. This ensures that the downstream machine learning models and visualization tools rely on high-quality, reliable data.

Technologies I work with include:

* **Python, SQL**
* **CI/CD pipelines**
* **AWS services**
* **Version control and deployment tools**

I also create data visualization and reporting solutions, allowing the team to monitor turbine performance, identify operational issues, and make data-driven decisions.

Overall, my daily work involves building ML models, engineering data pipelines, developing dashboards, and ensuring that high-quality energy data is ready for analysis. I find the work at Enertrag SE extremely interesting and impactful, as it directly contributes to the optimization of renewable energy production.


---

### **Professional Version – Agile Workflow & Team Collaboration**

As a Data Scientist, I work within an **Agile environment**, following established Agile methodologies throughout the development lifecycle. We work in **sprints**, with tasks clearly defined and assigned for each sprint cycle. Our team uses **Jira boards** to organize tasks, track progress, and ensure smooth collaboration.

Code quality is an important part of our workflow. We conduct **peer code reviews**, provide feedback to one another, and follow best practices to maintain high standards across our projects. This collaborative review process helps us continuously improve our codebase and share knowledge within the team.

I am comfortable working both **independently** and as part of a **team**, depending on the requirements of the situation. When working with colleagues, I believe in open and transparent communication. I like to discuss ideas, evaluate the pros and cons of different approaches, and collectively decide on the solution that is best for the team and the company.

I always aim to maintain a positive working environment and proactively try to **avoid conflicts** by addressing concerns early, listening to others, and promoting constructive collaboration.

---

### **Professional Version – Motivation & Background**

My academic background has been completely focused on **data**. Throughout my studies and my professional journey, I have developed a strong and genuine passion for working with data. All of my experience revolves around data-related fields, and I truly enjoy building **machine learning models**, **data models**, **ETL pipelines**, and **dashboards or visualization reports**.

I am highly motivated by opportunities that allow me to work deeply with data—whether it involves analysis, engineering, or machine learning. I enjoy exploring complex datasets, solving real-world problems, and creating meaningful data-driven solutions. This passion is what drives me to continue growing in the field and to seek roles where I can further expand my skills and contribute value through data.

---




